 # -*- coding: utf-8 -*-
"""
Created on Fri Jul 30 21:03:36 2021

@author: charlie
"""
from jumpy import codebase
import numpy as np
import os
import pathlib
from matplotlib import pyplot as plt


def calc_dis(a, b):
  return((((a[0]-b[0])**2)+((a[1]-b[1])**2))**0.5)

def calc_area(n):
  return(sum(n)-len(n)*min(n))


# Specify your video name and target pose class to count the repetitions.
# Open the video.
import cv2

def Counting(video_path):
    input_path = './Video/'+video_path
    video_cap = cv2.VideoCapture(input_path)

    # Get some video parameters to generate output video with classificaiton.
    video_n_frames = video_cap.get(cv2.CAP_PROP_FRAME_COUNT)
    video_fps = video_cap.get(cv2.CAP_PROP_FPS)
    video_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    video_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Initilize tracker, classifier and counter.
    # Do that before every video as all of them have state.
    from mediapipe.python.solutions import pose as mp_pose


    # Initialize tracker.
    pose_tracker = mp_pose.Pose(upper_body_only=False)

    print("A-------------------A")
    print(pathlib.Path().absolute())
    print("A-------------------A")
    pose_samples_folder = './jumpy/Jump_csvs_out'
    pose_embedder = codebase.FullBodyPoseEmbedder()

    pose_classifier = codebase.PoseClassifier(
        pose_samples_folder=pose_samples_folder,
        pose_embedder=pose_embedder,
        top_n_by_max_distance=30,
        top_n_by_mean_distance=10)


    # Run classification on a video.
    import os
    import tqdm
    from mediapipe.python.solutions import drawing_utils as mp_drawing
    import matplotlib.pyplot as plt
    import statistics
    import talib                        #EMA
    # Open output video. // 07360541/Videoname
    out_video = cv2.VideoWriter('./Video_out/'+video_path, cv2.VideoWriter_fourcc(*'mp4v'), video_fps, (video_width, video_height))

    frame_idx = 0
    output_frame = None
    n=count=flag=0
    x=[]
    x1=[]
    n_repeats=[]
    list_n=np.array(x)
    test=0
    dis_heel=[]
    avg_heel=0
    path = 'output.txt'
    f = open(path, 'w')
    with tqdm.tqdm(total=video_n_frames, position=0, leave=True) as pbar:
      while True:
        # Get next frame of the video.
        success, input_frame = video_cap.read()
        if not success:
          break
          
        # Run pose tracker.
        input_frame = cv2.cvtColor(input_frame, cv2.COLOR_BGR2RGB)
        result = pose_tracker.process(image=input_frame)
        pose_landmarks = result.pose_landmarks

        # Draw pose prediction.
        output_frame = input_frame.copy()
        if pose_landmarks is not None:
          mp_drawing.draw_landmarks(
          image=output_frame,
          landmark_list=pose_landmarks,
          connections=mp_pose.POSE_CONNECTIONS)
        
        if pose_landmarks is not None:
          # Get landmarks.
          frame_height, frame_width = output_frame.shape[0], output_frame.shape[1]
          pose_landmarks = np.array([[lmk.x * frame_width, lmk.y * frame_height, lmk.z * frame_width]
                                     for lmk in pose_landmarks.landmark], dtype=np.float32)
          assert pose_landmarks.shape == (33, 3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)

        dis_heel.append(calc_dis(pose_landmarks[29], pose_landmarks[30]))
        test=nk=video_height-((pose_landmarks[23][1]+pose_landmarks[24][1])/2)
        
        dis_thumb = abs(pose_landmarks[15][1]-pose_landmarks[16][1])
        pose_classification = pose_classifier(pose_landmarks)
        #print(str(pose_classification))
        
        
        if(frame_idx==0):
          n=nk
          
        if(pose_classification.get('jump', 0)<5):   
          x1.append(video_height)    
          flag=0
          n_repeats.clear()
          
          
        else:
          x1.append(0)    
          list_n=np.append(list_n, nk)
          if(len(list_n)>=4):
            ema=talib.EMA(list_n, 4)
            list_n=np.delete(list_n, 0)
            #x.append(ema[4])
            nk=ema[3]
           if(n<=nk):
        if(flag==1):
          if(len(n_repeats)>=4 and calc_area(n_repeats)>=10 and dis_thumb <= 50):
            #f.write('count='+str(count)+', len='+str(len(n_repeats))+', calc_area='+str(calc_area(n_repeats))+'\n')
            #x+=n_repeats
            #x+=[0, 0, 0]
            #dis_func(count, n_repeats)
            #area=calc_triangle_area(count, n_repeats)
            #list_area.append(area)
            count+=1
            #for i in n_repeats:
            #   xx.append(i)
            n_repeats.clear()
          flag=0
      else:
        flag=1
          n_repeats.append(nk)
          n=nk

        # Draw classification plot and repetition counter.
        # Save the output frame.
        #cv2.putText(output_frame, 'pose=' +str(pose_classification), (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 1, cv2.LINE_AA)
        cv2.putText(output_frame, 'frame=' +str(frame_idx), (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 1, cv2.LINE_AA)
        #cv2.putText(output_frame, 'count=' +str(count), (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 1, cv2.LINE_AA)
        cv2.putText(output_frame, 'count=' +str(count), (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 1, cv2.LINE_AA)
        out_video.write(cv2.cvtColor(np.array(output_frame), cv2.COLOR_RGB2BGR))
        #cv2.imwrite('img/'+str(frame_idx)+'.jpg', cv2.cvtColor(np.array(output_frame), cv2.COLOR_RGB2BGR))

        frame_idx += 1
        pbar.update()
     
    print("=================================")
    print("video counted")
    print("=================================")    
    # Close output video.
    out_video.release()

    # Release MediaPipe resources.
    pose_tracker.close()
    f.close()
    return count